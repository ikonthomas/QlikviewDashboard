{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark ETL to_Parquet",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikonthomas/SchoolPlay/blob/master/Spark_ETL_to_Parquet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Ls0-ze2LOAlV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Perfect code using PySpark**"
      ]
    },
    {
      "metadata": {
        "id": "zcY7WJxxN-z5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sc = SparkContext(appName=\"CSV2Parquet\")\n",
        "    sqlContext = SQLContext(sc)\n",
        "    \n",
        "    schema = StructType([\n",
        "            StructField(\"col1\", IntegerType(), True),\n",
        "            StructField(\"col2\", IntegerType(), True),\n",
        "            StructField(\"col3\", StringType(), True),\n",
        "            StructField(\"col4\", StringType(), True),\n",
        "            StructField(\"col5\", StringType(), True),\n",
        "            StructField(\"col6\", DoubleType(), True)])\n",
        "    \n",
        "    rdd = sc.textFile(\"/home/thomas/Desktop/input.csv\").map(lambda line: line.split(\",\"))\n",
        "    df = sqlContext.createDataFrame(rdd, schema)\n",
        "    df.write.parquet('/home/thomas/Desktop/input-parquet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9x6VN_YSOWO4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**OR as below**"
      ]
    },
    {
      "metadata": {
        "id": "hd5MONl6Ohh8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.types import *\n",
        "import sys\n",
        "\n",
        "sc = SparkContext(appName=\"CSV2Parquet\")\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"col1\", StringType(), True),\n",
        "    StructField(\"col2\", StringType(), True),\n",
        "    StructField(\"col3\", StringType(), True),\n",
        "    StructField(\"col4\", StringType(), True),\n",
        "    StructField(\"col5\", StringType(), True)])\n",
        "rdd = sc.textFile('/input.csv').map(lambda line: line.split(\",\"))\n",
        "df = sqlContext.createDataFrame(rdd, schema)\n",
        "df.write.parquet('/output.parquet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3TauXrMWMuzr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following code is an example using spark2.0. Reading is much faster than inferSchema option. Spark 2.0 convert into parquet file in much more efficient than spark1.6.\n"
      ]
    },
    {
      "metadata": {
        "id": "I3DgXvezMrpr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import org.apache.spark.sql.types._\n",
        "var df = StructType(Array(StructField(\"timestamp\", StringType, true),StructField(\"site\", StringType, true),StructField(\"requests\", LongType, true) ))\n",
        "df = spark.read\n",
        "          .schema(df)\n",
        "          .option(\"header\", \"true\")\n",
        "          .option(\"delimiter\", \"\\t\")\n",
        "          .csv(\"/user/user/wikipedia/pageviews-by-second-tsv\")\n",
        "df.write.parquet(\"/user/user/wikipedia/pageviews-by-second-parquet\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RJpRHCG4M7vX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "using AWS cloudtrail"
      ]
    },
    {
      "metadata": {
        "id": "Ziu-oIjFIfHy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "val streamingETLQuery = cloudtrailEvents\n",
        "  .withColumn(\"date\", $\"timestamp\".cast(\"date\") // derive the date\n",
        "  .writeStream\n",
        "  .trigger(ProcessingTime(\"10 seconds\")) // check for files every 10s\n",
        "  .format(\"parquet\") // write as Parquet partitioned by date\n",
        "  .partitionBy(\"date\")\n",
        "  .option(\"path\", \"/cloudtrail\")\n",
        "  .option(\"checkpointLocation\", \"/cloudtrail.checkpoint/\")\n",
        "  .start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yGVaQCqYMqtd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}